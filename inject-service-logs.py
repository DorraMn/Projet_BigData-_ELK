import requests
import random
from datetime import datetime, timedelta
import json

# Configuration
ELASTICSEARCH_URL = "http://localhost:9200"
INDEX_NAME = "logs-ecommerce-2025.11.25"

# Donn√©es de test
SERVICES = [
    "payment-api",
    "order-service",
    "inventory-service",
    "notification-service",
    "user-service",
    "analytics-service"
]

PRODUCTS = [
    "Laptop Dell XPS 13",
    "iPhone 15 Pro",
    "Sony WH-1000XM5",
    "Samsung Galaxy S24",
    "MacBook Pro M3",
    "iPad Air",
    "AirPods Pro",
    "Nintendo Switch",
    "PlayStation 5",
    "Xbox Series X",
    "Canon EOS R6",
    "GoPro Hero 12",
    "Kindle Paperwhite",
    "Apple Watch Series 9",
    "Dyson V15"
]

CUSTOMERS = [
    "Marie Dubois", "Pierre Martin", "Sophie Bernard", "Luc Petit",
    "Emma Durand", "Thomas Moreau", "Julie Simon", "Antoine Laurent",
    "Camille Lefebvre", "Nicolas Roux", "Laura Fournier", "Alexandre Morel",
    "L√©a Girard", "Maxime Andr√©", "Chlo√© Mercier", "Hugo Blanc",
    "Manon Garcia", "Lucas Rodriguez", "Sarah Sanchez", "Julien Dupont"
]

PAYMENT_TYPES = ["credit_card", "debit_card", "paypal", "bank_transfer", "apple_pay"]
CATEGORIES = ["electronics", "clothing", "books", "home", "sports", "beauty"]
STATUSES = ["success", "failed", "pending"]

MESSAGES = {
    "success": [
        "Transaction completed successfully",
        "Payment processed",
        "Order confirmed",
        "Item shipped",
        "Delivery confirmed"
    ],
    "failed": [
        "Payment declined",
        "Insufficient funds",
        "Card expired",
        "Transaction timeout",
        "Authentication failed"
    ],
    "pending": [
        "Payment processing",
        "Awaiting confirmation",
        "Order in queue",
        "Verification required",
        "Pending approval"
    ]
}

def generate_log_entry(timestamp):
    """G√©n√®re une entr√©e de log al√©atoire"""
    status = random.choice(STATUSES)
    service = random.choice(SERVICES)
    
    log = {
        "@timestamp": timestamp.isoformat() + "Z",
        "service": service,
        "status": status,
        "message": random.choice(MESSAGES[status]),
        "product": random.choice(PRODUCTS),
        "customer_name": random.choice(CUSTOMERS),
        "payment_type": random.choice(PAYMENT_TYPES),
        "category": random.choice(CATEGORIES),
        "amount": round(random.uniform(10, 1000), 2),
        "transaction_id": f"TXN-{random.randint(100000, 999999)}",
        "ip_address": f"{random.randint(1, 255)}.{random.randint(1, 255)}.{random.randint(1, 255)}.{random.randint(1, 255)}",
        "user_agent": random.choice([
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/120.0.0.0",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) Safari/605.1.15",
            "Mozilla/5.0 (X11; Linux x86_64) Firefox/121.0"
        ]),
        "response_time_ms": random.randint(50, 2000)
    }
    
    return log

def bulk_insert_logs(num_logs=300):
    """Ins√®re des logs en masse dans Elasticsearch"""
    print(f"üöÄ G√©n√©ration de {num_logs} logs avec champ service...")
    
    # G√©n√©rer les logs sur les 7 derniers jours
    now = datetime.utcnow()
    bulk_data = []
    
    for i in range(num_logs):
        # Distribuer les logs sur les 7 derniers jours
        days_ago = random.randint(0, 7)
        hours_ago = random.randint(0, 23)
        minutes_ago = random.randint(0, 59)
        
        timestamp = now - timedelta(days=days_ago, hours=hours_ago, minutes=minutes_ago)
        log_entry = generate_log_entry(timestamp)
        
        # Format bulk API
        action = {"index": {"_index": INDEX_NAME}}
        bulk_data.append(json.dumps(action))
        bulk_data.append(json.dumps(log_entry))
    
    # Envoyer les donn√©es
    bulk_body = "\n".join(bulk_data) + "\n"
    
    try:
        response = requests.post(
            f"{ELASTICSEARCH_URL}/_bulk",
            headers={"Content-Type": "application/x-ndjson"},
            data=bulk_body,
            timeout=30
        )
        
        if response.status_code == 200:
            result = response.json()
            if result.get("errors"):
                print("‚ö†Ô∏è  Certaines insertions ont √©chou√©")
                failed = sum(1 for item in result.get("items", []) if item.get("index", {}).get("error"))
                print(f"   √âchecs: {failed}/{num_logs}")
            else:
                print(f"‚úÖ {num_logs} logs ins√©r√©s avec succ√®s!")
                
            # Afficher les statistiques
            print(f"\nüìä Statistiques:")
            print(f"   - Services: {', '.join(SERVICES)}")
            print(f"   - Produits: {len(PRODUCTS)} types")
            print(f"   - Clients: {len(CUSTOMERS)} personnes")
            print(f"   - P√©riode: 7 derniers jours")
        else:
            print(f"‚ùå Erreur: {response.status_code}")
            print(response.text[:500])
    
    except Exception as e:
        print(f"‚ùå Erreur lors de l'insertion: {e}")

def verify_data():
    """V√©rifie les donn√©es ins√©r√©es"""
    print("\nüîç V√©rification des donn√©es...")
    
    try:
        # Compter les logs
        response = requests.get(f"{ELASTICSEARCH_URL}/{INDEX_NAME}/_count")
        if response.status_code == 200:
            count = response.json()["count"]
            print(f"‚úÖ Total des logs: {count}")
        
        # Compter par service
        query = {
            "size": 0,
            "aggs": {
                "services": {
                    "terms": {
                        "field": "service.keyword",
                        "size": 10
                    }
                }
            }
        }
        
        response = requests.post(
            f"{ELASTICSEARCH_URL}/{INDEX_NAME}/_search",
            headers={"Content-Type": "application/json"},
            json=query
        )
        
        if response.status_code == 200:
            buckets = response.json()["aggregations"]["services"]["buckets"]
            print(f"\nüìä Logs par service:")
            for bucket in buckets:
                print(f"   - {bucket['key']}: {bucket['doc_count']} logs")
        
        # Compter par status
        query["aggs"] = {
            "statuses": {
                "terms": {
                    "field": "status.keyword",
                    "size": 10
                }
            }
        }
        
        response = requests.post(
            f"{ELASTICSEARCH_URL}/{INDEX_NAME}/_search",
            headers={"Content-Type": "application/json"},
            json=query
        )
        
        if response.status_code == 200:
            buckets = response.json()["aggregations"]["statuses"]["buckets"]
            print(f"\nüìä Logs par status:")
            for bucket in buckets:
                print(f"   - {bucket['key']}: {bucket['doc_count']} logs")
    
    except Exception as e:
        print(f"‚ùå Erreur lors de la v√©rification: {e}")

if __name__ == "__main__":
    print("="*60)
    print("üì¶ Injection de donn√©es avec champ service")
    print("="*60)
    
    # Ins√©rer 300 nouveaux logs
    bulk_insert_logs(300)
    
    # V√©rifier les donn√©es
    verify_data()
    
    print("\n" + "="*60)
    print("‚úÖ Injection termin√©e!")
    print("üåê Testez la recherche: http://localhost:8000/search")
    print("="*60)
